{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ›  Integrating transformers with fastai for multiclass classification\nBefore beginning the implementation, note that integrating ``transformers`` within ``fastai`` can be done in multiple different ways. For that reason, I decided to bring simple solutions, that are the most generic and flexible. More precisely, I try to make the minimum of modification in both libraries while making them compatible with the maximum amount of transformer architectures.\n\nNote that in addition to this NoteBook and the [Medium article](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376), I made another version available on my GitHub(TODO add link).","metadata":{}},{"cell_type":"markdown","source":"## Libraries Installation\nBefore starting the implementation, you will need to install the ``fastai`` and ``transformers`` libraries. To do so, just follow the instructions [here](https://github.com/fastai/fastai/blob/master/README.md#installation) and [here](https://github.com/huggingface/transformers#installation).\n\nIn Kaggle, the ``fastai`` library is already installed. So you just have to instal ``transformers`` with :","metadata":{}},{"cell_type":"code","source":"!pip install transformers==2.5.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The current versions of the fastai and transformers libraries are respectively 1.0.58 and 2.5.1.","metadata":{}},{"cell_type":"code","source":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ðŸŽ¬ The exampleÂ task\nThe chosen task is a multi-class text classification on [Movie Reviews](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview).\n\nFor each text movie review, the model has to predict a label for the sentiment. We evaluate the outputs of the model on classification accuracy. The sentiment labels are:\n* 0 â†’ Negative\n* 1 â†’ Somewhat negative\n* 2 â†’ Neutral\n* 3 â†’ Somewhat positive\n* 4 â†’ Positive\n\nThe data is loaded into a ``DataFrame`` using ``pandas``.","metadata":{}},{"cell_type":"code","source":"\ntrain = pd.read_csv(\"../input/comve-subtask-a/train_B.csv\")\ntest = pd.read_csv(\"../input/comve-subtask-a/test_B.csv\")\nprint(train.shape,test.shape)\ntrain.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment label.","metadata":{}},{"cell_type":"markdown","source":"## Main transformers classes\nIn ``transformers``, each model architecture is associated with 3 main types of classes:\n* A **model class** to load/store a particular pre-train model.\n* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n* A **configuration class** to load/store the configuration of a particular model.\n\nFor example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) for the **configuration class**.Â \n\nIn order to switch easily between classes â€Š-â€Š each related to a specific model type â€Š-â€Š I created a dictionary that allows loading the correct classes by just specifying the correct model type name.","metadata":{}},{"cell_type":"code","source":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name,Â ...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model/tokenizer/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https://huggingface.co/transformers/pretrained_models.html#pretrained-models).","metadata":{}},{"cell_type":"code","source":"# Parameters\nseed = 42\nuse_fp16 = False\nbs = 32\n\n# model_type = 'roberta'\n# pretrained_model_name = 'roberta-base'\n\nmodel_type = 'bert'\npretrained_model_name='bert-base-uncased'\n\n# model_type = 'distilbert'\n# pretrained_model_name = 'distilbert-base-uncased'\n\n#model_type = 'xlm'\n#pretrained_model_name = 'xlm-clm-enfr-1024'\n\n# model_type = 'xlnet'\n# pretrained_model_name = 'xlnet-base-cased'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print the available values for ``pretrained_model_name`` (shortcut names) corresponding to the ``model_type`` used.","metadata":{}},{"cell_type":"code","source":"model_class.pretrained_model_archive_map.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is worth noting that in this case, we use the ``transformers`` library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types areÂ :\n* BERT (from Google)\n* XLNet (from Google/CMU)\n* XLM (from Facebook)\n* RoBERTa (from Facebook)\n* DistilBERT (from HuggingFace)\n\nHowever, if you want to go furtherâ€Š-â€Šby implementing another type of model or NLP taskâ€Š-â€Šthis tutorial still an excellent starter.","metadata":{}},{"cell_type":"markdown","source":"## Util function","metadata":{}},{"cell_type":"markdown","source":"Function to set the seed for generating random numbers.","metadata":{}},{"cell_type":"code","source":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data pre-processing\n\nTo match pre-training, we have to format the model input sequence in a specific format.\nTo do so, you have to first **tokenize** and then **numericalize** the texts correctly.\nThe difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-processâ€Š-â€Š**tokenization** & **numericalization**â€Š-â€Šthan the pre-process used during the pre-train part.\nFortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n\nIn the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \nAs you will see in the ``DataBunch`` implementation, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n\n``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n\nLet's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function.\n\n### Custom Tokenizer\nThis part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names.\nTo resume, if we look attentively at the ``fastai`` implementation, we notice thatÂ :\n1. The [``TokenizeProcessor`` object](https://docs.fast.ai/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n2. The [``Tokenizer`` object](https://docs.fast.ai/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n3. The [``BaseTokenizer`` object](https://docs.fast.ai/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) â†’ List[str]`` that take a text ``t`` and returns the list of its tokens.\n\nTherefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function.\n","metadata":{}},{"cell_type":"code","source":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this implementation, be carefull about 3 things :\n1. As we are not using RNN, we have to limit the sequence length to the model input size.\n2. Most of the models require special tokens placed at the beginning and end of the sequences.\n3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with ``add_prefix_space`` set to ``True``.\n\nBelow, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the [HuggingFace documentation](https://huggingface.co/transformers/) in each model section.\n\n    bert:       [CLS] + tokens + [SEP] + padding\n\n    roberta:    [CLS] + prefix_space + tokens + [SEP] + padding\n    \n    distilbert: [CLS] + tokens + [SEP] + padding\n\n    xlm:        [CLS] + tokens + [SEP] + padding\n\n    xlnet:      padding + tokens + [SEP] + [CLS]\n    \nIt is worth noting that we don't add padding in this part of the implementation.Â \nAs we will see later, ``fastai`` manage it automatically during the creation of the ``DataBunch``.","metadata":{}},{"cell_type":"markdown","source":"### Custom Numericalizer\n\nIn ``fastai``, [``NumericalizeProcessor``  object](https://docs.fast.ai/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https://docs.fast.ai/text.transform.html#Vocab). \nFrom this analyse, we suggest two ways to adapt the fastai numericalizer:\n1. You can, like decribed in the [Dev Sharma's article](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Section *1. Setting Up the Tokenizer*), retreive the list of tokens and create a ``Vocab`` object.\n2. Create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.\n\nEven if the first solution seems to be simpler, ``Transformers`` does not provide, for all models, a straightforward way to retreive his list of tokens. \nTherefore, I implemented the second solution, which runs for each model type.\nIt consists of using the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` in respectively ``numericalize`` and ``textify``.","metadata":{}},{"cell_type":"code","source":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NB: The functions ``__gestate__`` and ``__setstate__`` allow the functions [export](https://docs.fast.ai/basic_train.html#Learner.export) and [load_learner](https://docs.fast.ai/basic_train.html#load_learner) to work correctly with ``TransformersVocab``.","metadata":{}},{"cell_type":"markdown","source":"### Custom processor\nNow that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer.","metadata":{}},{"cell_type":"code","source":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up the Databunch\nFor the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding.\n\nAs mentioned in the HuggingFace documentation, BERT, RoBERTa, XLM and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left.","metadata":{}},{"cell_type":"code","source":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\nprint(tokens)\nids = transformer_tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\ntransformer_tokenizer.convert_ids_to_tokens(ids)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is multible ways to create a DataBunch, in our implementation, we use [the data block API](https://docs.fast.ai/data_block.html#The-data-block-API), which gives more flexibility.","metadata":{}},{"cell_type":"code","source":"databunch = (TextList.from_df(train, cols='sent', processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols= 'label')\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check batch and tokenizer :","metadata":{}},{"cell_type":"code","source":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check batch and numericalizer :","metadata":{}},{"cell_type":"code","source":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom model\nAs mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits.Â \nOne way to access them is to create a custom model.","metadata":{}},{"cell_type":"code","source":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        # attention_mask\n        # Mask to avoid performing attention on padding token indices.\n        # Mask values selected in ``[0, 1]``:\n        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        logits = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]   \n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) (Section: *Initializing the Learner*) the ``num_labels`` argument.","metadata":{}},{"cell_type":"code","source":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 2\nconfig.use_bfloat16 = use_fp16\nprint(config)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LearnerÂ : Custom Optimizer / CustomÂ Metric\nIn ``pytorch-transformers``, HuggingFace had implemented two specific optimizers â€Š-â€Š BertAdam and OpenAIAdam â€Š-â€Š that have been replaced by a single AdamW optimizer.\nThis optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\nIt is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.\n","metadata":{}},{"cell_type":"code","source":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discriminative Fine-tuning and Gradual unfreezing (Optional)\nTo use **discriminative layer training** and **gradual unfreezing**, ``fastai`` provides one tool that allows to \"split\" the structure model into groups. An instruction to perform that \"split\" is described in the fastai documentation [here](https://docs.fast.ai/basic_train.html#Discriminative-layer-training).\n\nUnfortunately,  the model architectures are too different to create a unique generic function that can \"split\" all the model types in a convenient way. Thereby, you will have to implement a custom \"split\" for each different model architecture.\n\nFor example, if we use the RobBERTa model and that we observe his architecture by making ``print(learner.model)``.","metadata":{}},{"cell_type":"code","source":"print(learner.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can decide to divide the model in 14 blocksÂ :\n* 1 Embedding\n* 12 transformer\n* 1 classifier\n\nIn this case, we can split our model in this wayÂ :","metadata":{}},{"cell_type":"code","source":"# For DistilBERT\n# list_layers = [learner.model.transformer.distilbert.embeddings,\n#                learner.model.transformer.distilbert.transformer.layer[0],\n#                learner.model.transformer.distilbert.transformer.layer[1],\n#                learner.model.transformer.distilbert.transformer.layer[2],\n#                learner.model.transformer.distilbert.transformer.layer[3],\n#                learner.model.transformer.distilbert.transformer.layer[4],\n#                learner.model.transformer.distilbert.transformer.layer[5],\n#                learner.model.transformer.pre_classifier]\n\n# For xlnet-base-cased\n# list_layers = [learner.model.transformer.transformer.word_embedding,\n#               learner.model.transformer.transformer.layer[0],\n#               learner.model.transformer.transformer.layer[1],\n#               learner.model.transformer.transformer.layer[2],\n#               learner.model.transformer.transformer.layer[3],\n#               learner.model.transformer.transformer.layer[4],\n#               learner.model.transformer.transformer.layer[5],\n#               learner.model.transformer.transformer.layer[6],\n#               learner.model.transformer.transformer.layer[7],\n#               learner.model.transformer.transformer.layer[8],\n#               learner.model.transformer.transformer.layer[9],\n#               learner.model.transformer.transformer.layer[10],\n#               learner.model.transformer.transformer.layer[11],\n#               learner.model.transformer.sequence_summary]\n\n# For roberta-base\n# list_layers = [learner.model.transformer.roberta.embeddings,\n#               learner.model.transformer.roberta.encoder.layer[0],\n#               learner.model.transformer.roberta.encoder.layer[1],\n#               learner.model.transformer.roberta.encoder.layer[2],\n#               learner.model.transformer.roberta.encoder.layer[3],\n#               learner.model.transformer.roberta.encoder.layer[4],\n#               learner.model.transformer.roberta.encoder.layer[5],\n#               learner.model.transformer.roberta.encoder.layer[6],\n#               learner.model.transformer.roberta.encoder.layer[7],\n#               learner.model.transformer.roberta.encoder.layer[8],\n#               learner.model.transformer.roberta.encoder.layer[9],\n#               learner.model.transformer.roberta.encoder.layer[10],\n#               learner.model.transformer.roberta.encoder.layer[11],\n#               learner.model.transformer.roberta.pooler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check groups : ","metadata":{}},{"cell_type":"code","source":"learner.split(list_layers)\nnum_groups = len(learner.layer_groups)\nprint('Learner split in',num_groups,'groups')\nprint(learner.layer_groups)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that I didn't found any document that has studied the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment.","metadata":{}},{"cell_type":"markdown","source":"## Train\nNow we can finally use all the fastai build-in features to train our model. Like the ULMFiT method, we will use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and **gradually unfreeze the model**.","metadata":{}},{"cell_type":"code","source":"learner.save('untrain')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('untrain');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore, we first freeze all the groups but the classifier withÂ :","metadata":{}},{"cell_type":"code","source":"learner.freeze_to(-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We check which layer are trainable.","metadata":{}},{"cell_type":"code","source":"learner.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For **Slanted Triangular Learning Rates** you have to use the function ``one_cycle``. For more information please check the fastai documentation [here](https://docs.fast.ai/callbacks.one_cycle.html).Â \n\nTo use our ``one_cycle`` we will need an optimum learning rate. We can find this learning rate by using a learning rate finder which can be called by using ``lr_find``.","metadata":{}},{"cell_type":"code","source":"learner.lr_find()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.recorder.plot(skip_end=10,suggestion=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will pick a value a bit before the minimum, where the loss still improves. Here 2x10^-3 seems to be a good value.\n\nNext we will use ``fit_one_cycle`` with the chosen learning rate as the maximum learning rate. ","metadata":{}},{"cell_type":"code","source":"learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.save('first_cycle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('first_cycle');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then unfreeze the second group of layers and repeat the operations.","metadata":{}},{"cell_type":"code","source":"learner.freeze_to(-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 1e-5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note here that we use slice to create separate learning rate for each group.","metadata":{}},{"cell_type":"code","source":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.save('second_cycle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('second_cycle');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.freeze_to(-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.save('third_cycle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('third_cycle');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we unfreeze all the groups.","metadata":{}},{"cell_type":"code","source":"learner.unfreeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now, you can predict examples with:","metadata":{}},{"cell_type":"code","source":"#         Learner.validate\n#         Learner.get_preds\n#         Learner.predict\n#         Learner.show_results\n#         Learner.no_logging\n#         Learner.loss_not_reduced","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.validate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.get_preds()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export Learner\nIn order to export and load the learner you can do these operations:","metadata":{}},{"cell_type":"code","source":"learner.export(file = 'transformer.pkl');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/working'\nexport_learner = load_learner(path, file = 'transformer.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned [here](https://docs.fast.ai/basic_train.html#load_learner), you have to be careful that each custom classes - like ``TransformersVocab`` - are first defined before executing ``load_learner``.","metadata":{}},{"cell_type":"code","source":"export_learner.predict('This is the worst movie of 2020')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating prediction\nNow that the model is trained, we want to generate predictions from the test dataset.\n\nAs specified in Keita Kurita's [article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/), as the function ``get_preds`` does not return elements in order by default, you will have to resort the elements into their correct order.","metadata":{}},{"cell_type":"code","source":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]\n\ntest_preds = get_preds_as_nparray(DatasetType.Test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(DATA_ROOT / 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(test_preds,axis=1)\nsample_submission.to_csv(\"predictions.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We check the order.","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='predictions.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now submit our predictions to Kaggle !  In our example, without playing too much with the parameters, we get a score of 0.70059, which leads us to the 5th position on the leaderboard! ","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nIn this NoteBook, I explain how to combine the ``transformers`` library with the beloved ``fastai`` library. It aims to make you understand where to look and modify both libraries to make them work together. Likely, it allows you to use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and even **Gradual Unfreezing**. As a result, without even tunning the parameters, you can obtain rapidly state-of-the-art results.\n\nThis year, the transformers became an essential tool to NLP. Because of that, I think that pre-trained transformers architectures will be integrated soon to future versions of fastai. Meanwhile, this tutorial is a good starter.\n\nI hope you enjoyed this first article and found it useful.Â \nThanks for reading and don't hesitate in leaving questions or suggestions.\n","metadata":{}},{"cell_type":"markdown","source":"# References\n* Hugging Face, Transformers GitHub (Nov 2019), [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n* Fast.ai, Fastai documentation (Nov 2019), [https://docs.fast.ai/text.html](https://docs.fast.ai/text.html)\n* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)\n* Keita Kurita's articleÂ : [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)Â (May 2019)\n* Dev Sharma's articleÂ : [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)","metadata":{}}]}