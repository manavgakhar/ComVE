{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==2.5.1","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:01.064366Z","iopub.execute_input":"2021-05-28T05:50:01.064663Z","iopub.status.idle":"2021-05-28T05:50:10.107753Z","shell.execute_reply.started":"2021-05-28T05:50:01.064611Z","shell.execute_reply":"2021-05-28T05:50:10.106898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path \n\nimport os\n\nimport torch\nimport torch.optim as optim\n\nimport random \n\n# fastai\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\n\n# transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\nfrom transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-28T05:50:10.112862Z","iopub.execute_input":"2021-05-28T05:50:10.114995Z","iopub.status.idle":"2021-05-28T05:50:15.267541Z","shell.execute_reply.started":"2021-05-28T05:50:10.114943Z","shell.execute_reply":"2021-05-28T05:50:15.266791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The current versions of the fastai and transformers libraries are respectively 1.0.58 and 2.5.1.","metadata":{}},{"cell_type":"code","source":"import fastai\nimport transformers\nprint('fastai version :', fastai.__version__)\nprint('transformers version :', transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:15.26911Z","iopub.execute_input":"2021-05-28T05:50:15.269426Z","iopub.status.idle":"2021-05-28T05:50:15.282212Z","shell.execute_reply.started":"2021-05-28T05:50:15.269378Z","shell.execute_reply":"2021-05-28T05:50:15.28084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain = pd.read_csv(\"../input/comve-subtask-a/train_A.csv\")\ntest = pd.read_csv(\"../input/comve-subtask-a/test_A.csv\")\nprint(train.shape,test.shape)\ntrain.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-05-28T05:50:15.284128Z","iopub.execute_input":"2021-05-28T05:50:15.284602Z","iopub.status.idle":"2021-05-28T05:50:15.377501Z","shell.execute_reply.started":"2021-05-28T05:50:15.284544Z","shell.execute_reply":"2021-05-28T05:50:15.376878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment label.","metadata":{}},{"cell_type":"markdown","source":"## Main transformers classes\nIn ``transformers``, each model architecture is associated with 3 main types of classes:\n* A **model class** to load/store a particular pre-train model.\n* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n* A **configuration class** to load/store the configuration of a particular model.\n\nFor example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) for the **configuration class**. \n\nIn order to switch easily between classes  -  each related to a specific model type  -  I created a dictionary that allows loading the correct classes by just specifying the correct model type name.","metadata":{}},{"cell_type":"code","source":"MODEL_CLASSES = {\n    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n}","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:15.379191Z","iopub.execute_input":"2021-05-28T05:50:15.379431Z","iopub.status.idle":"2021-05-28T05:50:15.386586Z","shell.execute_reply.started":"2021-05-28T05:50:15.379388Z","shell.execute_reply":"2021-05-28T05:50:15.383894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name, ...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model/tokenizer/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https://huggingface.co/transformers/pretrained_models.html#pretrained-models).","metadata":{}},{"cell_type":"code","source":"# Parameters\nseed = 42\nuse_fp16 = False\nbs = 32\n\n# model_type = 'roberta'\n# pretrained_model_name = 'roberta-large'\n\nmodel_type = 'roberta'\npretrained_model_name='roberta-large'\n\n# model_type = 'distilbert'\n# pretrained_model_name = 'distilbert-base-uncased'\n\n# model_type = 'xlm'\n# pretrained_model_name = 'xlm-clm-enfr-1024'\n\n# model_type = 'xlnet'\n# pretrained_model_name = 'xlnet-base-cased'","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:38.830068Z","iopub.execute_input":"2021-05-28T05:50:38.830384Z","iopub.status.idle":"2021-05-28T05:50:38.837272Z","shell.execute_reply.started":"2021-05-28T05:50:38.830328Z","shell.execute_reply":"2021-05-28T05:50:38.836503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:40.154124Z","iopub.execute_input":"2021-05-28T05:50:40.154443Z","iopub.status.idle":"2021-05-28T05:50:40.158774Z","shell.execute_reply.started":"2021-05-28T05:50:40.154391Z","shell.execute_reply":"2021-05-28T05:50:40.157604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print the available values for ``pretrained_model_name`` (shortcut names) corresponding to the ``model_type`` used.","metadata":{}},{"cell_type":"code","source":"model_class.pretrained_model_archive_map.keys()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:42.32845Z","iopub.execute_input":"2021-05-28T05:50:42.328741Z","iopub.status.idle":"2021-05-28T05:50:42.333898Z","shell.execute_reply.started":"2021-05-28T05:50:42.328691Z","shell.execute_reply":"2021-05-28T05:50:42.333105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is worth noting that in this case, we use the ``transformers`` library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types are :\n* BERT (from Google)\n* XLNet (from Google/CMU)\n* XLM (from Facebook)\n* RoBERTa (from Facebook)\n* DistilBERT (from HuggingFace)\n\nHowever, if you want to go further - by implementing another type of model or NLP task - this tutorial still an excellent starter.","metadata":{}},{"cell_type":"markdown","source":"## Util function","metadata":{}},{"cell_type":"markdown","source":"Function to set the seed for generating random numbers.","metadata":{}},{"cell_type":"code","source":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:56.943181Z","iopub.execute_input":"2021-05-28T05:50:56.943507Z","iopub.status.idle":"2021-05-28T05:50:56.951901Z","shell.execute_reply.started":"2021-05-28T05:50:56.943457Z","shell.execute_reply":"2021-05-28T05:50:56.9511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:50:58.70008Z","iopub.execute_input":"2021-05-28T05:50:58.700396Z","iopub.status.idle":"2021-05-28T05:50:58.708833Z","shell.execute_reply.started":"2021-05-28T05:50:58.700341Z","shell.execute_reply":"2021-05-28T05:50:58.708004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data pre-processing\n\nTo match pre-training, we have to format the model input sequence in a specific format.\nTo do so, you have to first **tokenize** and then **numericalize** the texts correctly.\nThe difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-process - **tokenization** & **numericalization** - than the pre-process used during the pre-train part.\nFortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n\nIn the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \nAs you will see in the ``DataBunch`` implementation, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n\n``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n\nLet's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function.\n\n### Custom Tokenizer\nThis part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names.\nTo resume, if we look attentively at the ``fastai`` implementation, we notice that :\n1. The [``TokenizeProcessor`` object](https://docs.fast.ai/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n2. The [``Tokenizer`` object](https://docs.fast.ai/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n3. The [``BaseTokenizer`` object](https://docs.fast.ai/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) → List[str]`` that take a text ``t`` and returns the list of its tokens.\n\nTherefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function.\n","metadata":{}},{"cell_type":"code","source":"class TransformersBaseTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.max_len\n        self.model_type = model_type\n\n    def __call__(self, *args, **kwargs): \n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n        CLS = self._pretrained_tokenizer.cls_token\n        SEP = self._pretrained_tokenizer.sep_token\n        if self.model_type in ['roberta']:\n            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n            tokens = [CLS] + tokens + [SEP]\n        else:\n            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n            if self.model_type in ['xlnet']:\n                tokens = tokens + [SEP] +  [CLS]\n            else:\n                tokens = [CLS] + tokens + [SEP]\n        return tokens","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:00.676637Z","iopub.execute_input":"2021-05-28T05:51:00.676925Z","iopub.status.idle":"2021-05-28T05:51:00.686041Z","shell.execute_reply.started":"2021-05-28T05:51:00.676876Z","shell.execute_reply":"2021-05-28T05:51:00.685325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\ntransformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\nfastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:01.746146Z","iopub.execute_input":"2021-05-28T05:51:01.746481Z","iopub.status.idle":"2021-05-28T05:51:03.98158Z","shell.execute_reply.started":"2021-05-28T05:51:01.746432Z","shell.execute_reply":"2021-05-28T05:51:03.980553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this implementation, be carefull about 3 things :\n1. As we are not using RNN, we have to limit the sequence length to the model input size.\n2. Most of the models require special tokens placed at the beginning and end of the sequences.\n3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with ``add_prefix_space`` set to ``True``.\n\nBelow, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the [HuggingFace documentation](https://huggingface.co/transformers/) in each model section.\n\n    bert:       [CLS] + tokens + [SEP] + padding\n\n    roberta:    [CLS] + prefix_space + tokens + [SEP] + padding\n    \n    distilbert: [CLS] + tokens + [SEP] + padding\n\n    xlm:        [CLS] + tokens + [SEP] + padding\n\n    xlnet:      padding + tokens + [SEP] + [CLS]\n    \nIt is worth noting that we don't add padding in this part of the implementation. \nAs we will see later, ``fastai`` manage it automatically during the creation of the ``DataBunch``.","metadata":{}},{"cell_type":"markdown","source":"### Custom Numericalizer\n\nIn ``fastai``, [``NumericalizeProcessor``  object](https://docs.fast.ai/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https://docs.fast.ai/text.transform.html#Vocab). \nFrom this analyse, we suggest two ways to adapt the fastai numericalizer:\n1. You can, like decribed in the [Dev Sharma's article](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Section *1. Setting Up the Tokenizer*), retreive the list of tokens and create a ``Vocab`` object.\n2. Create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.\n\nEven if the first solution seems to be simpler, ``Transformers`` does not provide, for all models, a straightforward way to retreive his list of tokens. \nTherefore, I implemented the second solution, which runs for each model type.\nIt consists of using the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` in respectively ``numericalize`` and ``textify``.","metadata":{}},{"cell_type":"code","source":"class TransformersVocab(Vocab):\n    def __init__(self, tokenizer: PreTrainedTokenizer):\n        super(TransformersVocab, self).__init__(itos = [])\n        self.tokenizer = tokenizer\n    \n    def numericalize(self, t:Collection[str]) -> List[int]:\n        \"Convert a list of tokens `t` to their ids.\"\n        return self.tokenizer.convert_tokens_to_ids(t)\n        #return self.tokenizer.encode(t)\n\n    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n        \"Convert a list of `nums` to their tokens.\"\n        nums = np.array(nums).tolist()\n        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)\n    \n    def __getstate__(self):\n        return {'itos':self.itos, 'tokenizer':self.tokenizer}\n\n    def __setstate__(self, state:dict):\n        self.itos = state['itos']\n        self.tokenizer = state['tokenizer']\n        self.stoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.itos)})","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:03.98325Z","iopub.execute_input":"2021-05-28T05:51:03.983709Z","iopub.status.idle":"2021-05-28T05:51:03.995618Z","shell.execute_reply.started":"2021-05-28T05:51:03.983523Z","shell.execute_reply":"2021-05-28T05:51:03.994738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NB: The functions ``__gestate__`` and ``__setstate__`` allow the functions [export](https://docs.fast.ai/basic_train.html#Learner.export) and [load_learner](https://docs.fast.ai/basic_train.html#load_learner) to work correctly with ``TransformersVocab``.","metadata":{}},{"cell_type":"markdown","source":"### Custom processor\nNow that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer.","metadata":{}},{"cell_type":"code","source":"transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\nnumericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n\ntokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n\ntransformer_processor = [tokenize_processor, numericalize_processor]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:04.944185Z","iopub.execute_input":"2021-05-28T05:51:04.944791Z","iopub.status.idle":"2021-05-28T05:51:04.957445Z","shell.execute_reply.started":"2021-05-28T05:51:04.9446Z","shell.execute_reply":"2021-05-28T05:51:04.954412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up the Databunch\nFor the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding.\n\nAs mentioned in the HuggingFace documentation, BERT, RoBERTa, XLM and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left.","metadata":{}},{"cell_type":"code","source":"pad_first = bool(model_type in ['xlnet'])\npad_idx = transformer_tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:06.251621Z","iopub.execute_input":"2021-05-28T05:51:06.251928Z","iopub.status.idle":"2021-05-28T05:51:06.25776Z","shell.execute_reply.started":"2021-05-28T05:51:06.251874Z","shell.execute_reply":"2021-05-28T05:51:06.256784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\nprint(tokens)\nids = transformer_tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\ntransformer_tokenizer.convert_ids_to_tokens(ids)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:06.520464Z","iopub.execute_input":"2021-05-28T05:51:06.520734Z","iopub.status.idle":"2021-05-28T05:51:06.529013Z","shell.execute_reply.started":"2021-05-28T05:51:06.520688Z","shell.execute_reply":"2021-05-28T05:51:06.528213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is multible ways to create a DataBunch, in our implementation, we use [the data block API](https://docs.fast.ai/data_block.html#The-data-block-API), which gives more flexibility.","metadata":{}},{"cell_type":"code","source":"databunch = (TextList.from_df(train, cols='sent', processor=transformer_processor)\n             .split_by_rand_pct(0.1,seed=seed)\n             .label_from_df(cols= 'label')\n             .add_test(test)\n             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:12.270381Z","iopub.execute_input":"2021-05-28T05:51:12.27068Z","iopub.status.idle":"2021-05-28T05:51:17.208055Z","shell.execute_reply.started":"2021-05-28T05:51:12.270633Z","shell.execute_reply":"2021-05-28T05:51:17.207135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check batch and tokenizer :","metadata":{}},{"cell_type":"code","source":"print('[CLS] token :', transformer_tokenizer.cls_token)\nprint('[SEP] token :', transformer_tokenizer.sep_token)\nprint('[PAD] token :', transformer_tokenizer.pad_token)\ndatabunch.show_batch()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:17.210036Z","iopub.execute_input":"2021-05-28T05:51:17.210356Z","iopub.status.idle":"2021-05-28T05:51:23.556203Z","shell.execute_reply.started":"2021-05-28T05:51:17.210306Z","shell.execute_reply":"2021-05-28T05:51:23.555381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check batch and numericalizer :","metadata":{}},{"cell_type":"code","source":"print('[CLS] id :', transformer_tokenizer.cls_token_id)\nprint('[SEP] id :', transformer_tokenizer.sep_token_id)\nprint('[PAD] id :', pad_idx)\ntest_one_batch = databunch.one_batch()[0]\nprint('Batch shape : ',test_one_batch.shape)\nprint(test_one_batch)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:23.55758Z","iopub.execute_input":"2021-05-28T05:51:23.557896Z","iopub.status.idle":"2021-05-28T05:51:24.535004Z","shell.execute_reply.started":"2021-05-28T05:51:23.557847Z","shell.execute_reply":"2021-05-28T05:51:24.533876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom model\nAs mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits. \nOne way to access them is to create a custom model.","metadata":{}},{"cell_type":"code","source":"# defining our model architecture \nclass CustomTransformerModel(nn.Module):\n    def __init__(self, transformer_model: PreTrainedModel):\n        super(CustomTransformerModel,self).__init__()\n        self.transformer = transformer_model\n        \n    def forward(self, input_ids, attention_mask=None):\n        \n        # attention_mask\n        # Mask to avoid performing attention on padding token indices.\n        # Mask values selected in ``[0, 1]``:\n        # ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        attention_mask = (input_ids!=pad_idx).type(input_ids.type()) \n        \n        logits = self.transformer(input_ids,\n                                  attention_mask = attention_mask)[0]   \n        return logits","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:51:24.536984Z","iopub.execute_input":"2021-05-28T05:51:24.537287Z","iopub.status.idle":"2021-05-28T05:51:24.548698Z","shell.execute_reply.started":"2021-05-28T05:51:24.537224Z","shell.execute_reply":"2021-05-28T05:51:24.547375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) (Section: *Initializing the Learner*) the ``num_labels`` argument.","metadata":{}},{"cell_type":"code","source":"config = config_class.from_pretrained(pretrained_model_name)\nconfig.num_labels = 2\nconfig.use_bfloat16 = use_fp16\nprint(config)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:54:47.462715Z","iopub.execute_input":"2021-05-28T05:54:47.463018Z","iopub.status.idle":"2021-05-28T05:54:47.827401Z","shell.execute_reply.started":"2021-05-28T05:54:47.462971Z","shell.execute_reply":"2021-05-28T05:54:47.826533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n\ncustom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:54:48.437827Z","iopub.execute_input":"2021-05-28T05:54:48.438122Z","iopub.status.idle":"2021-05-28T05:54:57.674827Z","shell.execute_reply.started":"2021-05-28T05:54:48.438072Z","shell.execute_reply":"2021-05-28T05:54:57.67411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learner : Custom Optimizer / Custom Metric\nIn ``pytorch-transformers``, HuggingFace had implemented two specific optimizers  -  BertAdam and OpenAIAdam  -  that have been replaced by a single AdamW optimizer.\nThis optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\nIt is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.\n","metadata":{}},{"cell_type":"code","source":"from fastai.callbacks import *\nfrom transformers import AdamW\nfrom functools import partial\n\nCustomAdamW = partial(AdamW, correct_bias=False)\n\nlearner = Learner(databunch, \n                  custom_transformer_model, \n                  opt_func = CustomAdamW, \n                  metrics=[accuracy, error_rate])\n\n# Show graph of learner stats and metrics after each epoch.\nlearner.callbacks.append(ShowGraph(learner))\n\n# Put learn in FP16 precision mode. --> Seems to not working\nif use_fp16: learner = learner.to_fp16()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:59:09.769752Z","iopub.execute_input":"2021-05-28T05:59:09.770056Z","iopub.status.idle":"2021-05-28T05:59:09.813022Z","shell.execute_reply.started":"2021-05-28T05:59:09.770005Z","shell.execute_reply":"2021-05-28T05:59:09.812239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discriminative Fine-tuning and Gradual unfreezing (Optional)\nTo use **discriminative layer training** and **gradual unfreezing**, ``fastai`` provides one tool that allows to \"split\" the structure model into groups. An instruction to perform that \"split\" is described in the fastai documentation [here](https://docs.fast.ai/basic_train.html#Discriminative-layer-training).\n\nUnfortunately,  the model architectures are too different to create a unique generic function that can \"split\" all the model types in a convenient way. Thereby, you will have to implement a custom \"split\" for each different model architecture.\n\nFor example, if we use the RobBERTa model and that we observe his architecture by making ``print(learner.model)``.","metadata":{}},{"cell_type":"code","source":"print(learner.model)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:56:36.317882Z","iopub.execute_input":"2021-05-28T05:56:36.318184Z","iopub.status.idle":"2021-05-28T05:56:36.326278Z","shell.execute_reply.started":"2021-05-28T05:56:36.318132Z","shell.execute_reply":"2021-05-28T05:56:36.325305Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can decide to divide the model in 14 blocks :\n* 1 Embedding\n* 12 transformer\n* 1 classifier\n\nIn this case, we can split our model in this way :","metadata":{}},{"cell_type":"code","source":"# For DistilBERT\n# list_layers = [learner.model.transformer.distilbert.embeddings,\n#                learner.model.transformer.distilbert.transformer.layer[0],\n#                learner.model.transformer.distilbert.transformer.layer[1],\n#                learner.model.transformer.distilbert.transformer.layer[2],\n#                learner.model.transformer.distilbert.transformer.layer[3],\n#                learner.model.transformer.distilbert.transformer.layer[4],\n#                learner.model.transformer.distilbert.transformer.layer[5],\n#                learner.model.transformer.pre_classifier]\n\n# For xlnet-base-cased\n# list_layers = [learner.model.transformer.transformer.word_embedding,\n#               learner.model.transformer.transformer.layer[0],\n#               learner.model.transformer.transformer.layer[1],\n#               learner.model.transformer.transformer.layer[2],\n#               learner.model.transformer.transformer.layer[3],\n#               learner.model.transformer.transformer.layer[4],\n#               learner.model.transformer.transformer.layer[5],\n#               learner.model.transformer.transformer.layer[6],\n#               learner.model.transformer.transformer.layer[7],\n#               learner.model.transformer.transformer.layer[8],\n#               learner.model.transformer.transformer.layer[9],\n#               learner.model.transformer.transformer.layer[10],\n#               learner.model.transformer.transformer.layer[11],\n#               learner.model.transformer.sequence_summary]\n\n# For roberta-base\n# list_layers = [learner.model.transformer.roberta.embeddings,\n#               learner.model.transformer.roberta.encoder.layer[0],\n#               learner.model.transformer.roberta.encoder.layer[1],\n#               learner.model.transformer.roberta.encoder.layer[2],\n#               learner.model.transformer.roberta.encoder.layer[3],\n#               learner.model.transformer.roberta.encoder.layer[4],\n#               learner.model.transformer.roberta.encoder.layer[5],\n#               learner.model.transformer.roberta.encoder.layer[6],\n#               learner.model.transformer.roberta.encoder.layer[7],\n#               learner.model.transformer.roberta.encoder.layer[8],\n#               learner.model.transformer.roberta.encoder.layer[9],\n#               learner.model.transformer.roberta.encoder.layer[10],\n#               learner.model.transformer.roberta.encoder.layer[11],\n#               learner.model.transformer.roberta.pooler]","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:54:58.083539Z","iopub.execute_input":"2021-05-28T05:54:58.084037Z","iopub.status.idle":"2021-05-28T05:54:58.092944Z","shell.execute_reply.started":"2021-05-28T05:54:58.083988Z","shell.execute_reply":"2021-05-28T05:54:58.092392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check groups : ","metadata":{}},{"cell_type":"code","source":"# learner.split(list_layers)\n# num_groups = len(learner.layer_groups)\n# print('Learner split in',num_groups,'groups')\n# print(learner.layer_groups)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:54:58.095312Z","iopub.execute_input":"2021-05-28T05:54:58.09575Z","iopub.status.idle":"2021-05-28T05:54:58.106563Z","shell.execute_reply.started":"2021-05-28T05:54:58.095576Z","shell.execute_reply":"2021-05-28T05:54:58.105917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that I didn't found any document that has studied the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment.","metadata":{}},{"cell_type":"markdown","source":"## Train\nNow we can finally use all the fastai build-in features to train our model. Like the ULMFiT method, we will use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and **gradually unfreeze the model**.","metadata":{}},{"cell_type":"code","source":"learner.save('untrain')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('untrain');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore, we first freeze all the groups but the classifier with :","metadata":{}},{"cell_type":"code","source":"learner.freeze_to(-1)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T06:01:54.296423Z","iopub.execute_input":"2021-05-28T06:01:54.296745Z","iopub.status.idle":"2021-05-28T06:01:54.307544Z","shell.execute_reply.started":"2021-05-28T06:01:54.296696Z","shell.execute_reply":"2021-05-28T06:01:54.306551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We check which layer are trainable.","metadata":{}},{"cell_type":"code","source":"learner.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T06:01:55.464028Z","iopub.execute_input":"2021-05-28T06:01:55.464336Z","iopub.status.idle":"2021-05-28T06:01:56.330274Z","shell.execute_reply.started":"2021-05-28T06:01:55.464276Z","shell.execute_reply":"2021-05-28T06:01:56.329159Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For **Slanted Triangular Learning Rates** you have to use the function ``one_cycle``. For more information please check the fastai documentation [here](https://docs.fast.ai/callbacks.one_cycle.html). \n\nTo use our ``one_cycle`` we will need an optimum learning rate. We can find this learning rate by using a learning rate finder which can be called by using ``lr_find``.","metadata":{}},{"cell_type":"code","source":"learner.lr_find()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.recorder.plot(skip_end=10,suggestion=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will pick a value a bit before the minimum, where the loss still improves. Here 2x10^-3 seems to be a good value.\n\nNext we will use ``fit_one_cycle`` with the chosen learning rate as the maximum learning rate. ","metadata":{}},{"cell_type":"code","source":"learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.save('first_cycle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('first_cycle');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then unfreeze the second group of layers and repeat the operations.","metadata":{}},{"cell_type":"code","source":"learner.freeze_to(-2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 1e-5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note here that we use slice to create separate learning rate for each group.","metadata":{}},{"cell_type":"code","source":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.save('second_cycle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('second_cycle');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.freeze_to(-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.save('third_cycle')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(seed)\nlearner.load('third_cycle');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we unfreeze all the groups.","metadata":{}},{"cell_type":"code","source":"learner.unfreeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.fit_one_cycle(10, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T05:52:28.7695Z","iopub.execute_input":"2021-05-28T05:52:28.769793Z","iopub.status.idle":"2021-05-28T05:53:37.173503Z","shell.execute_reply.started":"2021-05-28T05:52:28.769742Z","shell.execute_reply":"2021-05-28T05:53:37.172168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now, you can predict examples with:","metadata":{}},{"cell_type":"code","source":"#         Learner.validate\n#         Learner.get_preds\n#         Learner.predict\n#         Learner.show_results\n#         Learner.no_logging\n#         Learner.loss_not_reduced","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.validate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learner.get_preds()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}